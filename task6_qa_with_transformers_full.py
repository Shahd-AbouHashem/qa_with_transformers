# -*- coding: utf-8 -*-
"""Task6_QA_with_Transformers_FULL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17HD8tLLAljXMQ5c3AvMLvSl47gJ2nIQz

# Task 6: Question Answering with Transformers
"""

!pip install transformers datasets evaluate streamlit --quiet

from transformers import pipeline
from datasets import load_dataset
import evaluate
import pandas as pd
import torch
import os
from IPython.display import display, Markdown

""" Load QA pipeline"""

model_name = "distilbert-base-uncased-distilled-squad"
qa_pipeline = pipeline("question-answering", model=model_name, tokenizer=model_name)

"""Custom context and question

"""

context = """
The Transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.
It relies entirely on self-attention mechanisms, dispensing with recurrence entirely, and is the foundation of models such as BERT and GPT.
"""
question = "Who introduced the Transformer architecture?"
result = qa_pipeline(question=question, context=context)
display(Markdown(f"**Answer:** {result['answer']} | **Score:** {result['score']:.2f}"))

"""Evaluate on subset of SQuAD with exact match and F1 score

"""

squad = load_dataset("squad", split="validation[:50]")
metric = evaluate.load("squad")

predictions = []
references = []

for item in squad:
    context = item['context']
    question = item['question']
    true_answers = item['answers']['text']
    result = qa_pipeline(question=question, context=context)
    predictions.append({"id": item['id'], "prediction_text": result['answer']})
    references.append({"id": item['id'], "answers": item['answers']})

scores = metric.compute(predictions=predictions, references=references)
print(f"Exact Match: {scores['exact_match']:.2f}%")
print(f"F1 Score: {scores['f1']:.2f}%")

from datasets import load_dataset
from transformers import pipeline
import evaluate

# Load the QA model
qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

# Load validation set first 50 examples
squad = load_dataset("squad", split="validation[:50]")

# Load SQuAD metric
metric = evaluate.load("squad")

# Prepare predictions and references
predictions = []
references = []

for example in squad:
    context = example["context"]
    question = example["question"]
    answers = example["answers"]
    result = qa_pipeline(question=question, context=context)

    predictions.append({
        "id": example["id"],
        "prediction_text": result["answer"]
    })
    references.append({
        "id": example["id"],
        "answers": answers
    })

results = metric.compute(predictions=predictions, references=references)
print(f"Evaluated on 50 samples from SQuAD v1.1")
print(f" Exact Match: {results['exact_match']:.2f}%")
print(f" F1 Score: {results['f1']:.2f}%")

"""Compare different models

"""

models = [
    "distilbert-base-uncased-distilled-squad",
    "bert-large-uncased-whole-word-masking-finetuned-squad",
    "deepset/roberta-base-squad2"
]
context = """
Transformers were introduced by Vaswani et al. and form the basis for models like BERT and GPT.
"""
question = "Who introduced Transformers?"
for model in models:
    qa = pipeline("question-answering", model=model, tokenizer=model)
    result = qa(question=question, context=context)
    print(f"Model: {model}")
    print(f"Answer: {result['answer']} | Score: {result['score']:.2f}\n")

!pip install streamlit pyngrok --quiet